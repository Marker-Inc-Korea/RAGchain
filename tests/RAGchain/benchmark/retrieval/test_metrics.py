import pytest

from RAGchain.benchmark.retrieval.metrics import Recall, RR, Precision, NDCG, DCG, Hole, TopKAccuracy, IDCG, IndDCG, \
    IndIDCG, AP, CG, EM_retrieval, F1

metric_instances = [Recall(), RR(), Precision(), NDCG(), DCG(), Hole(), TopKAccuracy(), IDCG(), IndDCG(), IndIDCG(),
                    AP(), CG(), EM_retrieval(), F1()]

metric_test_values = [
    ({"1": 1, "2": 1, "3": 1}, {"1": 1, "2": 1, "3": 1}, 3,
     [1.0, 1.0, 1.0, 1.0, 2.1309297535714578, 0.0, 1.0, 2.1309297535714578, 2.1309297535714578, 2.1309297535714578, 1.0,
      3.0, 1.0, 1.0]),
    ({"1": 1, "2": 1, "3": 1}, {"1": 1, "2": 1, "3": 1}, 2,
     [0.6666666666666666, 1.0, 1.0, 1.0, 1.6309297535714575, 0.0, 1.0, 1.6309297535714575, 1.6309297535714575,
      1.6309297535714575, 1.0, 2.0, 1.0, 0.8]),
    ({"1": 1, "2": 1, "3": 1}, {"1": 1, "2": 1, "3": 1}, 1,
     [0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5]),
    ({"1": 1, "2": 1, "3": 1, "4": 1}, {"1": 1, "2": 1, "3": 1}, 3,
     [0.75, 1.0, 1.0, 1.0, 2.1309297535714578, 0.0, 1.0, 2.1309297535714578, 2.1309297535714578, 2.1309297535714578,
      1.0, 3.0, 0.0, 0.8571428571428571]),
    ({"1": 1, "2": 1, "3": 1}, {"1": 1, "2": 1, "3": 1, "4": 1}, 3,
     [1.0, 1.0, 1.0, 1.0, 2.1309297535714578, 0.0, 1.0, 2.1309297535714578, 2.1309297535714578, 2.1309297535714578, 1.0,
      3.0, 0.0, 1.0]),
    ({"1": 1, "2": 1, "3": 1}, {"3": 1, "5": 1, "2": 1}, 3,
     [0.6666666666666666, 1.0, 0.6666666666666666, 0.9197207891481876, 1.5, 0.3333333333333333, 1.0, 1.6309297535714575,
      1.5, 1.6309297535714575, 0.8333333333333333, 2.0, 0.0, 0.6666666666666666]),
    # rank aware
    ({"1": 1, "2": 2, "3": 3}, {"1": 1, "2": 2, "3": 3}, 3,
     [1.0, 1.0, 1.0, 1.0, 4.7618595071429155, 0.0, 1.0, 4.7618595071429155, 9.392789260714373, 9.392789260714373, 1.0,
      6.0, 1.0, 1.0]),
    ({"1": 1, "2": 2, "3": 3}, {"1": 1, "2": 2, "3": 3}, 2,
     [0.6666666666666666, 1.0, 1.0, 1.0, 4.2618595071429155, 0.0, 1.0, 4.2618595071429155, 8.892789260714373,
      8.892789260714373, 1.0, 5.0, 1.0, 0.8]),
    ({"1": 1, "2": 2, "3": 3}, {"1": 1, "2": 2, "3": 3}, 1,
     [0.3333333333333333, 1.0, 1.0, 1.0, 3.0, 0.0, 1.0, 3.0, 7.0, 7.0, 1.0, 3.0, 1.0, 0.5]),
    ({"1": 1, "2": 2, "3": 3, "4": 4}, {"1": 1, "2": 2, "3": 3}, 3,
     [0.75, 1.0, 1.0, 1.0, 4.7618595071429155, 0.0, 1.0, 4.7618595071429155, 9.392789260714373, 9.392789260714373, 1.0,
      6.0, 0.0, 0.8571428571428571]),
    ({"1": 1, "2": 2, "3": 3, "4": 4}, {"1": 1, "2": 2, "3": 3}, 3,
     [0.75, 1.0, 1.0, 1.0, 4.7618595071429155, 0.0, 1.0, 4.7618595071429155, 9.392789260714373, 9.392789260714373, 1.0,
      6.0, 0.0, 0.8571428571428571]),
    ({"1": 1, "2": 2, "3": 3}, {"1": 1, "2": 2, "3": 3, "4": 4}, 3,
     [0.6666666666666666, 0.5, 0.6666666666666666, 0.6653152460429406, 2.8927892607143724, 0.3333333333333333, 1.0,
      4.2618595071429155, 5.9165082750002025, 8.892789260714373, 0.5833333333333333, 5.0, 0.0, 0.6666666666666666]),
    ({"1": 1, "2": 2, "3": 3}, {"3": 1, "5": 2, "2": 3}, 3,
     [0.6666666666666666, 1.0, 0.6666666666666666, 0.7309292742059024, 3.5, 0.3333333333333333, 1.0, 4.2618595071429155,
      6.5, 8.892789260714373, 0.8333333333333333, 5.0, 0.0, 0.6666666666666666]),
    pytest.param({"1": 1, "2": 1, "3": 1}, {"3": 1, "5": 1, "2": 1}, 4,
                 [0.6666666666666666, 1.0, 0.6666666666666666, 0.7309292742059024, 3.5, 0.3333333333333333, 1.0,
                  4.2618595071429155, 6.5, 8.892789260714373, 0.8333333333333333, 5.0, 0.0, 0.6666666666666666],
                 marks=pytest.mark.xfail)  # too larger k than preds
]


@pytest.mark.parametrize("qrels, preds, k,test_gold", metric_test_values)
def test_metrics(qrels, preds, k, test_gold):
    #qrels, preds, k = test_input
    epsilon = 0.0001
    for i in range(len(test_gold)):
        gold = test_gold[i]
        metric = metric_instances[i]
        score = metric.eval(qrels, preds, k)
        gap = score - gold
        assert abs(gap) < epsilon, f"{metric.metric_name}({score}) failed on {qrels} and {preds}, gap: {gap}"
